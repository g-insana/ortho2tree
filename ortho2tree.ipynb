{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ortho2tree pipeline\n",
    "### by Giuseppe Insana and William R Pearson\n",
    "### https://github.com/g-insana/ortho2tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time  # elapsed times\n",
    "import yaml  # for configuration files\n",
    "from multiprocessing import Pool, cpu_count, current_process, set_start_method\n",
    "from platform import platform\n",
    "\n",
    "from ortho2tree.o2t_buildtree import build_tree_for_orthogroup\n",
    "from ortho2tree.o2t_df import create_ortho_df, ortho_df_stats, read_ortho_df\n",
    "from ortho2tree.o2t_gc_integration import dump_prev_changes\n",
    "from ortho2tree.o2t_load import get_gc_df, get_panther_df\n",
    "from ortho2tree.o2t_output import (\n",
    "    clean_up_tempfiles,\n",
    "    combine_and_print_output,\n",
    "    output_headers,\n",
    ")\n",
    "from ortho2tree.o2t_scan_ndata import scan_ndata_file\n",
    "from ortho2tree.o2t_utils import (\n",
    "    check_all_files_exist,\n",
    "    elapsed_time,\n",
    "    eprint,\n",
    "    get_orthologs_df_from_pantherid,\n",
    "    get_sequences_uniparc,\n",
    "    get_sequences_swpread,\n",
    "    get_sequences_web,\n",
    ")\n",
    "\n",
    "sys.stderr.write(\"*** Ortho2tree pipeline ***\\n\")\n",
    "\n",
    "# for multithreading\n",
    "if sys.version_info >= (3, 8, 10) and platform().find(\"macOS\") != -1:\n",
    "    sys.stderr.write(\"INFO: Using fork MP context macos and py >=3.8.10\\n\")\n",
    "    try:\n",
    "        # NOTE: because spawn fails due to freeze_support\n",
    "        set_start_method(\"fork\")\n",
    "    except RuntimeError:\n",
    "        sys.stderr.write(\"NOTICE: context already set\\n\")\n",
    "\n",
    "# optional, for progressbar\n",
    "tqdm_available = True\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    tqdm_available = False\n",
    "\n",
    "# for oracle database connection\n",
    "try:\n",
    "    import cx_Oracle\n",
    "except ImportError:\n",
    "    sys.stderr.write(\"NOTICE: Oracle module not available\\n\")\n",
    "\n",
    "# optional, for connection engine\n",
    "sqlalchemy_available = True\n",
    "try:\n",
    "    import sqlalchemy\n",
    "except ImportError:\n",
    "    sqlalchemy_available = False\n",
    "    sys.stderr.write(\"NOTICE: sqlalchemy module not available\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: any or all of the following parameters can be overridden with the yaml configuration file config = { # # # Performance and UI options # # #\n",
    "config = {\n",
    "    \"threads\": 7,  # how many parallel threads?\n",
    "    \"progressbar\": True,  # show tqdm progressbar (if tqdm module is present)\n",
    "    \"print_stats\": True,  # print statistics about the df\n",
    "    # # # DB, API and Files options # # #\n",
    "    \"gc_from_sql\": True,  # use oracle db sql query for gc groups? (if false, get data from tsv file)\n",
    "    \"seq_from_sql\": True,  # use oracle db sql queries to retrieve sequences? (if false, get data via protein API)\n",
    "    \"use_uniparc_for_seq_retrieval\": True,  # otherwise use swpread\n",
    "    \"cache_sequences_flag\": True,  # if getting sequence data via protein API, should we cache the files?\n",
    "    \"cache_alignments_flag\": True,  # should we store and cache alignments?\n",
    "    \"cache_trees_flag\": True,  # should we store and cache trees?\n",
    "    \"create_pdf_files_flag\": True,  # should we create pdf files for suggestions? implies create lab_lt and faX files\n",
    "    \"create_pdf_files4confirmed_flag\": True,  # should we create pdf files also for confirmed canonical solutions?\n",
    "    \"panther_data_dir\": \"PANTHER_Sequence_Classification_files/\",  # where to store local copy of panther data?\n",
    "    \"fasta_dir\": \"fasta\",  # directory name where sequences cache can be stored\n",
    "    # \"geneid2refseq_mapfile\": \"refprots_geneid2refseq.csv.gz\",  # geneid 2 refseq mapping file\n",
    "    # \"missing_geneid_mapfile\": \"up_refseq_geneid_map95_90.tab\",  # missing geneid 2 up acc mapping file\n",
    "    # Note for following paths: dataset_name/ is the same name as the cfg file. e.g. 5taxa/ for 5taxa.cfg\n",
    "    \"n_data_dir\": \"n_data\",  # directory name where output n_data will be written to (under dataset/)\n",
    "    \"lab_data_dir\": \"lab_data\",  # directory name where output .lab_lt files will be written to (under dataset/)\n",
    "    \"aln_data_dir\": \"aln_data\",  # directory name where output alignments will optionally be written to (under dataset/)\n",
    "    \"tree_data_dir\": \"tree_data\",  # directory name where trees will optionally be written to (under dataset/)\n",
    "    \"pdf_data_dir\": \"pdf_data\",  # directory name where pdf files for suggestions will optionally be written to (under dataset/)\n",
    "    \"semaphores_dir\": \"processed\",  # directory name where to create semaphore files (under dataset/)\n",
    "    # # # DATA version options # # #\n",
    "    \"panther_version\": \"PTHR18.0\",  # Panther version\n",
    "    \"up_release\": \"2024_02\",  # UniProt Release version\n",
    "    # # # Parameters for the ANALYSIS # # #\n",
    "    \"min_taxa_threshold\": 3,  # minimum number of different organisms for building an alignment\n",
    "    \"taxa_threshold\": 3,  # number of different organisms that should be in the low-cost clade for acceptance (if taxa in tree more than this, otherwise use min_taxa_threshold)\n",
    "    \"tree_max_cost\": 0.05,  # exclude tree_to_ndata solutions with costs higher than this (default=0.05)\n",
    "    \"tree_drop_fact\": 1.5,  # improvement required to drop a taxon\n",
    "    \"gap_threshold\": 0,  # threshold for counting gaps\n",
    "    \"superfamily_level\": False,  # set to true to work at superfamily level\n",
    "    \"add_refseq\": False,  # whether we want to add refseq sequences to orthogroups\n",
    "    # # parameters used when scanning n_data2 files to select suggestions # #\n",
    "    \"min_delta_threshold\": 0.005,  # minimum cost difference for individual taxa\n",
    "    \"min_delta_sp_threshold\": 0.02,  # minimum cost difference for individual sp taxa\n",
    "    \"suggestion_score_difference\": -0.001,  # cost difference\n",
    "    \"suggestion_taxa_threshold\": 3,  # minimum number of taxa\n",
    "    \"suggestion_min_canon\": 1,  # minimum number of canonicals\n",
    "    \"suggestion_max_clade_cost\": 0.02,  # max clade cost\n",
    "    \"suggestion_only_zero_cost\": False,  # only consider suggestions with zero cost\n",
    "    \"suggestion_ranking_weights\": {\n",
    "        \"n_sp\": 16.0,\n",
    "        \"n_tax\": 4.0,\n",
    "        \"n_canon\": 0.0,\n",
    "        \"wn_canon\": 12.0,\n",
    "        \"scaled_prop_f\": 4.0,\n",
    "        \"scaled_p_cost\": 8.0,\n",
    "        \"p_len_diff\": 1.0,\n",
    "    },\n",
    "    \"suggestion_taxon_weights\": {\"HUMAN\": 2.0, \"MOUSE\": 2.0, \"RAT\": 1.0},\n",
    "    \"suggestion_taxon_weight_default\": 0.5,\n",
    "    \"skip_reprocessing_orthogroups\": False,  # set this to skip (re)processing of orthogroups, useful if only want to re-score existing n_data files\n",
    "    # # # OUTLIERS options # # #\n",
    "    \"remove_outliers\": True,  # should we remove sequences that have been flagged as outliers?\n",
    "    \"max_seqlen\": 10000,  # maximum sequence length, sequences beyond this length (and their isoforms) will not be considered\n",
    "    # std/mean based outlier detection:\n",
    "    # outliers are identified as those under /threshold_low/ or beyond /threshold_hi/ standard deviations from mean of sequence lengths\n",
    "    \"detect_outliers_with_mean_std\": False,\n",
    "    \"outliers_detection_threshold_std_lo\": 2,\n",
    "    \"outliers_detection_threshold_std_hi\": 2,\n",
    "    # median based outlier detection:\n",
    "    # outliers are identified as those with seqlen under /threshold_median_lo/ or beyond /threshold_median_hi/ times the median\n",
    "    \"detect_outliers_with_median\": False,\n",
    "    \"outliers_detection_threshold_median_lo\": 0.75,\n",
    "    \"outliers_detection_threshold_median_hi\": 2,\n",
    "    # quartile based outlier detection:\n",
    "    # outliers are identified as those with seqlen under /threshold_quart_lo/ times Q1 or beyond /threshold_quart_hi/ times the Q3 value\n",
    "    \"detect_outliers_with_quart\": False,\n",
    "    \"outliers_detection_threshold_quart_lo\": 0.5,\n",
    "    \"outliers_detection_threshold_quart_hi\": 2,\n",
    "    # canonical seqlen outlier detection:\n",
    "    # outliers are identified as those with seqlen under /threshold_can_lo/ times the can_min_len (min seqlen of canonicals in group) or beyond /threshold_can_hi/ times the can_max_len (max seqlen of canonicals in group)\n",
    "    \"outliers_detection_threshold_can_lo\": 0.5,\n",
    "    \"outliers_detection_threshold_can_hi\": 2,\n",
    "    # median + canonical seqlen outlier detection:\n",
    "    \"detect_outliers_with_median_and_can_lengths\": False,\n",
    "    # quartile + canonical seqlen outlier detection:\n",
    "    \"detect_outliers_with_quart_and_can_lengths\": True,\n",
    "    # # # DATAFRAME reading and caching # # #\n",
    "    \"dump_orthogroup_data\": True,  # do we want to dump the data into output files marked with a certain /outstamp/?\n",
    "    \"outstamp\": \"240102\",  # this will be appended to output file names\n",
    "    \"use_cached_orthogroup_data\": False,  # do we want to read cached dataframe from previously dumped files, marked with a certain /instamp/?\n",
    "    \"instamp\": \"240101\",  # this will be appended to input file names\n",
    "    # # # GCINTEGRATION # # #\n",
    "    \"sugg_file\": False,  # if a filename is specified, it will be read to simulate genecentric application of ortho2tree using previously created suggestions\n",
    "    \"prevgc_file\": False,  # if a filename is specified, it will be read and used for cumulative integrated changes for genecentric pipeline\n",
    "    \"allow_flipflop_before\": \"\",  # if set (e.g. '2023_05'), allow suggestions that represent a flip flip (reverting a previously made suggestion), but only if the previous suggestion was made on a release before the one specified here\n",
    "    # # # ORGANISMS definition # # #\n",
    "    # use panther organism names as values in the following dict, with tax_id as keys\n",
    "    \"tax2org\": {9606: \"human\", 10090: \"mouse\", 10116: \"rat\", 9913: \"cow\", 9615: \"dog\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1) Configuration and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration override from a yaml configuration file, if present and parsing of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"groups2run\"] = []\n",
    "config[\"debug_mode\"] = False\n",
    "config[\"ignore_cache\"] = False\n",
    "\n",
    "# if we are running in the notebook (ortho2tree.ipynb)\n",
    "if sys.argv[0].find(\"pykernel_launcher\") != -1:\n",
    "    # default set for notebook; override to test other sets\n",
    "    config[\"dataset_name\"] = \"qfomam\"\n",
    "else:  # we are running from the shell as ortho2tree.py\n",
    "    USAGE_EXAMPLE = \"\"\"\n",
    "    Examples:\n",
    "       -set=qfomam                                 #will do the analysis on the whole set\n",
    "       -set=qfomam -id=PTHR19918:SF1               #only for one orthogroup\n",
    "       -set=qfomam -id=PTHR19918:SF1 PTHR40139:SF1 #only for two orthogroups\n",
    "       -set=qfomam -file=list_of_ids.txt           #for a series of groups listed in a file\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter, epilog=USAGE_EXAMPLE\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-set\",\n",
    "        dest=\"dataset_name\",\n",
    "        required=True,\n",
    "        type=str,\n",
    "        help=\"set for the analysis. a file SET.cfg should be present\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        dest=\"debug_mode\",\n",
    "        required=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"print verbose/debug messages\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-nocache\",\n",
    "        dest=\"ignore_cache\",\n",
    "        required=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"do not use cache, re-create alignments/trees and do not save them\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-no_stats\",\n",
    "        dest=\"no_stats\",\n",
    "        required=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"do not print any stats on the dataframe\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-id\",\n",
    "        dest=\"single_group\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        nargs=\"+\",\n",
    "        help=\"to only work on one or few group(s)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-file\",\n",
    "        dest=\"list_filename\",\n",
    "        required=False,\n",
    "        type=argparse.FileType(\"r\"),\n",
    "        help=\"to work on a series of groups, from a file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-sugg\",\n",
    "        dest=\"sugg_file\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        help=\"to simulate integration of canonical suggestions reading a previosly generated changes file; note that file should be placed in the set main dir\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-prevgc\",\n",
    "        dest=\"prevgc_file\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        help=\"to integrate previosly generated changes file; note that file should be placed in the set main dir\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-outstamp\",\n",
    "        dest=\"outstamp\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        help=\"to name and timestamp the output files and the dumps; this overrides the outstamp parameter from the config\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    if args.single_group is not None and args.list_filename is not None:\n",
    "        eprint(\"    => ERROR: either pass -id or -file, not both\")\n",
    "        sys.exit(22)\n",
    "    if args.ignore_cache:\n",
    "        config[\"ignore_cache\"] = True\n",
    "    if args.debug_mode:\n",
    "        config[\"debug_mode\"] = True\n",
    "    if args.single_group is not None:\n",
    "        config[\"groups2run\"] = list(args.single_group)\n",
    "    if args.list_filename is not None:\n",
    "        for line in args.list_filename.readlines():\n",
    "            config[\"groups2run\"].append(line.rstrip())\n",
    "    if args.sugg_file is not None:\n",
    "        if os.path.isfile(os.path.join(args.dataset_name, args.sugg_file)):\n",
    "            eprint(\n",
    "                \"NOTICE: Suggestion file override from command line: {}\".format(\n",
    "                    args.sugg_file\n",
    "                )\n",
    "            )\n",
    "            # override default from config\n",
    "            config[\"sugg_file\"] = args.sugg_file\n",
    "        else:\n",
    "            eprint(\"    => ERROR: file specified as -sugg does not exist!\")\n",
    "            sys.exit(2)\n",
    "    if args.prevgc_file is not None:\n",
    "        if os.path.isfile(os.path.join(args.dataset_name, args.prevgc_file)):\n",
    "            eprint(\n",
    "                \"NOTICE: prevgc_file override from command line: {}\".format(\n",
    "                    args.prevgc_file\n",
    "                )\n",
    "            )\n",
    "            # override default from config\n",
    "            config[\"prevgc_file\"] = args.prevgc_file\n",
    "        else:\n",
    "            eprint(\"    => ERROR: file specified as -prevgc does not exist!\")\n",
    "            sys.exit(2)\n",
    "\n",
    "    config[\"dataset_name\"] = args.dataset_name\n",
    "\n",
    "config[\"dataset_maindir\"] = config[\"dataset_name\"]\n",
    "dataset_configfile = config[\"dataset_name\"] + \".cfg\"\n",
    "if os.path.isfile(dataset_configfile):\n",
    "    eprint(\n",
    "        \"NOTICE: Default configuration overridden by YAML file {}\".format(\n",
    "            dataset_configfile\n",
    "        )\n",
    "    )\n",
    "    with open(dataset_configfile, encoding=\"utf-8\") as fh:\n",
    "        config.update(yaml.safe_load(fh))\n",
    "\n",
    "# if we are running from script with argparse\n",
    "if sys.argv[0].find(\"pykernel_launcher\") == -1:\n",
    "    if args.outstamp is not None:\n",
    "        config[\"outstamp\"] = args.outstamp\n",
    "    if args.no_stats:\n",
    "        config[\"print_stats\"] = False\n",
    "else:  # we are in jupyter\n",
    "    if sys.version_info >= (3, 8, 10) and platform().find(\"macOS\") != -1:\n",
    "        eprint(\n",
    "            \"NOTICE: Forcing single thread mode due to multiprocessing issue with jupyter macOS py>=3.8.10\"\n",
    "        )\n",
    "        config[\"threads\"] = 1\n",
    "\n",
    "if not tqdm_available:\n",
    "    config[\"progressbar\"] = False\n",
    "\n",
    "eprint(\"\\nINFO: Configured parameters: {}\".format(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File definitions, creation of paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamped files:\n",
    "file_keys = [\n",
    "    \"orthogroup_df_cachefile\",\n",
    "    \"orthogroup_df_dumpfile\",  # to read/write the mapped and processed dataframe\n",
    "    \"prevgc_notfound_file\",  # entries no more present in gc but that were previous gc suggestions\n",
    "    \"prevgc_conflict_file\",  # conflicting suggestions being removed\n",
    "    \"groups_by_taxa_count_file\",  # list of groups and their sizes in number of taxa\n",
    "    \"groups_by_entry_count_file\",  # list of groups and their sizes in number of entries, excluding lowtaxa and outliers\n",
    "    \"low_taxa_groups_file\",  # list of groups with taxa size lower than threshold\n",
    "    \"tr_fragments_df_cachefile\",\n",
    "    \"tr_fragments_df_dumpfile\",  # to read or write trembl fragments\n",
    "    \"unmapped_canonicals_file\",  # list of canonicals not mapped\n",
    "]\n",
    "for key in file_keys:\n",
    "    if key.endswith(\"_cachefile\"):\n",
    "        config[key] = f\"{key[:-10]}_dump{config['instamp']}.gz\"\n",
    "    elif key.endswith(\"_dumpfile\"):\n",
    "        config[key] = f\"{key[:-9]}_dump{config['outstamp']}.gz\"\n",
    "    elif key in [\"unmapped_canonicals_file\", \"low_taxa_groups_file\"]:\n",
    "        config[key] = f\"{key[:-5]}{config['outstamp']}\"\n",
    "    else:  # compressed\n",
    "        config[key] = f\"{key[:-5]}{config['outstamp']}.gz\"\n",
    "\n",
    "\n",
    "# output intermediate files:\n",
    "config[\"output_keys\"] = [\"conflict\", \"changes\", \"confirm\", \"skipped\", \"gc\"]\n",
    "for key in config[\"output_keys\"]:\n",
    "    config[key + \"_outfile\"] = \"output_\" + key + config[\"outstamp\"]\n",
    "\n",
    "# headers for output files\n",
    "headers = output_headers(config=config)\n",
    "\n",
    "# taxa and tax_ids\n",
    "if config[\"taxa_threshold\"] > len(config[\"tax2org\"]):\n",
    "    eprint(\n",
    "        \"WARNING: the specified level taxa_threshold ({}) was higher than the number of species defined for the analysis ({}). Using the latter for threshold\".format(\n",
    "            config[\"taxa_threshold\"], len(config[\"tax2org\"])\n",
    "        )\n",
    "    )\n",
    "    config[\"taxa_threshold\"] = len(config[\"tax2org\"])\n",
    "eprint(\n",
    "    \"\\nINFO: Number of species defined for the analysis: {}, threshold for n_data printout: {}\".format(\n",
    "        len(config[\"tax2org\"]), config[\"taxa_threshold\"]\n",
    "    )\n",
    ")\n",
    "config[\"tax_ids\"] = set(config[\"tax2org\"].keys())\n",
    "\n",
    "# file paths creation\n",
    "if not os.path.exists(config[\"dataset_maindir\"]):\n",
    "    os.mkdir(config[\"dataset_maindir\"])\n",
    "\n",
    "# path join\n",
    "paths_to_join = [\n",
    "    \"n_data_dir\",\n",
    "    \"lab_data_dir\",\n",
    "    \"aln_data_dir\",\n",
    "    \"tree_data_dir\",\n",
    "    \"pdf_data_dir\",\n",
    "    \"semaphores_dir\",\n",
    "    \"unmapped_canonicals_file\",\n",
    "    \"orthogroup_df_dumpfile\",\n",
    "    \"tr_fragments_df_dumpfile\",\n",
    "    \"prevgc_notfound_file\",\n",
    "    \"prevgc_conflict_file\",\n",
    "    \"groups_by_taxa_count_file\",\n",
    "    \"groups_by_entry_count_file\",\n",
    "    \"low_taxa_groups_file\",\n",
    "    \"orthogroup_df_cachefile\",\n",
    "    \"tr_fragments_df_cachefile\",\n",
    "    \"gc_outfile\",\n",
    "    \"changes_outfile\",\n",
    "    \"confirm_outfile\",\n",
    "    \"skipped_outfile\",\n",
    "    \"conflict_outfile\",\n",
    "]\n",
    "for key in paths_to_join:\n",
    "    config[key] = os.path.join(config[\"dataset_maindir\"], config[key])\n",
    "\n",
    "# cached dataframe\n",
    "if config[\"use_cached_orthogroup_data\"] and not os.path.isfile(\n",
    "    config[\"orthogroup_df_cachefile\"]\n",
    "):\n",
    "    eprint(\"    => ERROR: no such file {}\".format(config[\"orthogroup_df_cachefile\"]))\n",
    "    sys.exit(2)\n",
    "\n",
    "\n",
    "# gc integration\n",
    "files_to_check = [\"sugg_file\", \"prevgc_file\"]\n",
    "for key in files_to_check:\n",
    "    file_path = config[key]\n",
    "    if file_path:\n",
    "        if file_path == \"False\":\n",
    "            config[key] = False\n",
    "        else:\n",
    "            config[key] = os.path.join(config[\"dataset_maindir\"], file_path)\n",
    "\n",
    "if config[\"ignore_cache\"]:\n",
    "    config[\"cache_alignments_flag\"] = False\n",
    "    config[\"cache_trees_flag\"] = False\n",
    "    config[\"cache_sequences_flag\"] = False\n",
    "\n",
    "directories_to_create = [\n",
    "    \"panther_data_dir\",\n",
    "    \"n_data_dir\",\n",
    "    \"lab_data_dir\",\n",
    "    \"aln_data_dir\",\n",
    "    \"tree_data_dir\",\n",
    "    \"semaphores_dir\",\n",
    "]\n",
    "\n",
    "config[\"create_lablt_files_flag\"] = False\n",
    "config[\"create_faX_files_flag\"] = False\n",
    "if config[\"create_pdf_files_flag\"]:\n",
    "    # the following files are needed to create the pdf files for the suggestions\n",
    "    config[\"create_lablt_files_flag\"] = True\n",
    "    config[\"create_faX_files_flag\"] = True\n",
    "    directories_to_create.append(\"pdf_data_dir\")\n",
    "\n",
    "if config[\"cache_sequences_flag\"]:\n",
    "    directories_to_create.append(\"fasta_dir\")\n",
    "\n",
    "for key in directories_to_create:\n",
    "    directory_path = config[key]\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.mkdir(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks for db access (UniProt only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"cx_Oracle\" not in sys.modules:\n",
    "    config[\"gc_from_sql\"] = False\n",
    "    config[\"seq_from_sql\"] = False\n",
    "\n",
    "config[\"db_connection\"] = \"\"\n",
    "if config[\"gc_from_sql\"]:\n",
    "    if not os.path.isfile(\"swpread_connection.pass\"):\n",
    "        config[\"gc_from_sql\"] = False\n",
    "    else:  # read connection details\n",
    "        with open(\n",
    "            \"swpread_connection.pass\",\n",
    "            \"r\",\n",
    "            encoding=\"utf-8\",  # for uniprotkb (format: username/password@dbname)\n",
    "        ) as f_in:\n",
    "            config[\"db_connection\"] = f_in.read().rstrip()\n",
    "    try:\n",
    "        cx_Oracle.connect(config[\"db_connection\"])\n",
    "    except cx_Oracle.DatabaseError as e:\n",
    "        print(\"WARNING: No db connection to uniprot for Genecentric data: {}\".format(e))\n",
    "        config[\"gc_from_sql\"] = False\n",
    "\n",
    "config[\"db_connection_uniparc\"] = \"\"\n",
    "if config[\"seq_from_sql\"]:\n",
    "    if not os.path.isfile(\"uatst_connection.pass\"):\n",
    "        config[\"seq_from_sql\"] = False\n",
    "    else:\n",
    "        with open(\n",
    "            \"uatst_connection.pass\", \"r\", encoding=\"utf-8\"  # for uniparc\n",
    "        ) as f_in:\n",
    "            config[\"db_connection_uniparc\"] = f_in.read().rstrip()\n",
    "        try:\n",
    "            cx_Oracle.connect(config[\"db_connection_uniparc\"])\n",
    "        except cx_Oracle.DatabaseError as e:\n",
    "            print(\n",
    "                \"WARNING: no db connection to uniparc: '{}'; reverting to API for sequence retrieval\".format(\n",
    "                    e\n",
    "                )\n",
    "            )\n",
    "            config[\"seq_from_sql\"] = False\n",
    "\n",
    "\n",
    "def dbconnect(dbconnection):\n",
    "    \"\"\"\n",
    "    Connect to db and returns cx_Oracle instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return cx_Oracle.connect(dbconnection)\n",
    "    except PermissionError as excep:\n",
    "        eprint(\n",
    "            \"    => ERROR: You have not specified the correct username/password@dbname\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        eprint(str(excep), file=sys.stderr)\n",
    "        sys.exit(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_wrapper(config={}):\n",
    "    \"\"\"\n",
    "    wrapper to have get_sequences appropriate to current setup (via db or via web api)\n",
    "    \"\"\"\n",
    "\n",
    "    if config[\"seq_from_sql\"]:  # get sequences via database access\n",
    "        if config[\"use_uniparc_for_seq_retrieval\"]:\n",
    "            db_connection_details = config[\"db_connection_uniparc\"]\n",
    "            get_sequences_db_function = get_sequences_uniparc\n",
    "            db_database_name = \"UNIPARC\"\n",
    "        else:\n",
    "            db_connection_details = config[\"db_connection\"]\n",
    "            get_sequences_db_function = get_sequences_swpread\n",
    "            db_database_name = \"SWPREAD\"\n",
    "\n",
    "        if sqlalchemy_available:\n",
    "            eprint(\n",
    "                \"We will retrieve sequences using {} database via sqlalchemy\".format(\n",
    "                    db_database_name\n",
    "                )\n",
    "            )\n",
    "            engine = sqlalchemy.create_engine(\n",
    "                \"oracle+cx_oracle://{}\".format(db_connection_details.replace(\"/\", \":\")),\n",
    "                pool_size=config[\"threads\"],\n",
    "                max_overflow=2,\n",
    "            )\n",
    "\n",
    "            def _wrapper(accessions, orthoid=\"\", format=\"Dict\", config={}):\n",
    "                try:\n",
    "                    db_conn = engine.pool.connect()\n",
    "                    return get_sequences_db_function(\n",
    "                        accessions,\n",
    "                        orthoid=orthoid,\n",
    "                        format=format,\n",
    "                        config=config,\n",
    "                        db_conn=db_conn.driver_connection,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    eprint(\"ERROR fetching sequences: {}\".format(e))\n",
    "                finally:\n",
    "                    db_conn.close()\n",
    "\n",
    "        else:\n",
    "            eprint(\n",
    "                \"We will retrieve sequences using {} database via cx_Oracle\".format(\n",
    "                    db_database_name\n",
    "                )\n",
    "            )\n",
    "\n",
    "            db_user, db_tmp = db_connection_details.split(\"/\", 1)\n",
    "            db_pwd, db_name = db_tmp.split(\"@\", 1)\n",
    "            oracle_pool = cx_Oracle.SessionPool(\n",
    "                user=db_user,\n",
    "                password=db_pwd,\n",
    "                dsn=db_name,\n",
    "                min=1,\n",
    "                max=config[\"threads\"] + 2,\n",
    "            )\n",
    "            db_conn = oracle_pool.acquire()\n",
    "            oracle_pool.release(db_conn)\n",
    "\n",
    "            def _wrapper(accessions, orthoid=\"\", format=\"Dict\", config={}):\n",
    "                try:\n",
    "                    db_conn = oracle_pool.acquire()\n",
    "                    return get_sequences_uniparc(\n",
    "                        accessions,\n",
    "                        orthoid=orthoid,\n",
    "                        format=format,\n",
    "                        config=config,\n",
    "                        db_conn=db_conn,\n",
    "                    )\n",
    "                except cx_Oracle.DatabaseError as exc:\n",
    "                    err = exc.args\n",
    "                    eprint(\"Oracle-Error-Code:\", err.code)\n",
    "                    eprint(\"Oracle-Error-Message:\", err.message)\n",
    "                except Exception as e:\n",
    "                    eprint(\"ERROR fetching sequences: {}\".format(e))\n",
    "                finally:\n",
    "                    oracle_pool.release(db_conn)\n",
    "\n",
    "    else:  # get sequences via online API retrieval\n",
    "        eprint(\"We will retrieve sequences using WEB API\")\n",
    "\n",
    "        def _wrapper(accessions, orthoid=\"\", format=\"Dict\", config={}):\n",
    "            return get_sequences_web(\n",
    "                accessions, orthoid=orthoid, format=format, config=config\n",
    "            )\n",
    "\n",
    "    return _wrapper\n",
    "\n",
    "\n",
    "get_sequences = get_sequences_wrapper(config=config)\n",
    "\n",
    "\n",
    "def build_tree_for_orthogroup_wrapper(orthoid):\n",
    "    \"\"\"\n",
    "    wrapper to avoid reprocessing or to force reprocessing\n",
    "    \"\"\"\n",
    "    semaphore_file = os.path.join(config[\"semaphores_dir\"], orthoid + \".done\")\n",
    "    if FORCE_REPROCESS:\n",
    "        build_tree_for_orthogroup(\n",
    "            orthoid,\n",
    "            ortho_df=ortho_df,\n",
    "            verbose=VERBOSE_PROCESS,\n",
    "            config=config,\n",
    "            get_sequences_fn=get_sequences,\n",
    "        )\n",
    "        if not os.path.isfile(semaphore_file):\n",
    "            open(semaphore_file, \"a\").close()  # touch semaphore file\n",
    "    else:\n",
    "        if not os.path.isfile(semaphore_file):  # to avoid recreating\n",
    "            build_tree_for_orthogroup(\n",
    "                orthoid,\n",
    "                ortho_df=ortho_df,\n",
    "                verbose=VERBOSE_PROCESS,\n",
    "                config=config,\n",
    "                get_sequences_fn=get_sequences,\n",
    "            )\n",
    "            open(semaphore_file, \"a\").close()  # touch semaphore file\n",
    "\n",
    "\n",
    "def scan_ndata_file_wrapper(orthoid):\n",
    "    \"\"\"\n",
    "    multithread wrapper in order to write into temporary files, one for each thread\n",
    "    \"\"\"\n",
    "    output_data = {}\n",
    "    (\n",
    "        output_data[\"gc\"],\n",
    "        output_data[\"changes\"],\n",
    "        output_data[\"confirm\"],\n",
    "        output_data[\"skipped\"],\n",
    "        output_data[\"conflict\"],\n",
    "    ) = scan_ndata_file(\n",
    "        orthoid,\n",
    "        ortho_df=ortho_df,\n",
    "        prevgc_df=prevgc_df,\n",
    "        config=config,\n",
    "        get_sequences_fn=get_sequences,\n",
    "    )\n",
    "\n",
    "    if current_process().name == \"MainProcess\":\n",
    "        workerid = \"XX\"\n",
    "    else:\n",
    "        workerid = \"{}\".format(current_process().name.split(\"-\")[1])\n",
    "\n",
    "    for key in config[\"output_keys\"]:\n",
    "        if output_data[key] != \"\":\n",
    "            tempfile = \"{}..{}\".format(config[key + \"_outfile\"], workerid)\n",
    "            with open(tempfile, \"a\", encoding=\"utf-8\") as outputfh:\n",
    "                outputfh.write(output_data[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.1: Load of Panther Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config[\"use_cached_orthogroup_data\"]:\n",
    "    start_secs = time.time()\n",
    "    panther_df, config[\"tax2oscode\"] = get_panther_df(config=config)\n",
    "    eprint(\n",
    "        \"\\n*** Part 2.1 workflow completed {} -- Elapsed: {} --\\n\".format(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "            elapsed_time(start_secs),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.2: Load of Genecentric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config[\"use_cached_orthogroup_data\"]:\n",
    "    start_secs = time.time()\n",
    "    db_conn = None\n",
    "    if config[\"gc_from_sql\"]:\n",
    "        db_conn = dbconnect(config[\"db_connection\"])\n",
    "    gc_df = get_gc_df(config=config, db_conn=db_conn)\n",
    "\n",
    "    eprint(\n",
    "        \"Loaded {} gc accessions, of which {} canonicals\".format(\n",
    "            len(gc_df), len(gc_df[gc_df[\"is_canonical\"]])\n",
    "        )\n",
    "    )\n",
    "    eprint(\n",
    "        \"\\n*** Part 2.2 workflow completed {} -- Elapsed: {} --\\n\".format(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "            elapsed_time(start_secs),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_secs = time.time()\n",
    "if config[\"use_cached_orthogroup_data\"]:\n",
    "    ortho_df, prevgc_df = read_ortho_df(config=config)\n",
    "else:\n",
    "    ortho_df, prevgc_df = create_ortho_df(\n",
    "        panther_df, gc_df, config=config, get_sequences_fn=get_sequences\n",
    "    )\n",
    "    # optionally, free memory by deleting df no more needed\n",
    "    del panther_df\n",
    "    del gc_df\n",
    "\n",
    "if len(config[\"groups2run\"]) == 0:  # unless we are doing single groups\n",
    "    # print stats and filter out groups lower than min_taxa\n",
    "    ortho_df = ortho_df_stats(ortho_df, config=config)\n",
    "\n",
    "orthogroups = set(ortho_df[\"pantherid\"].drop_duplicates().values)\n",
    "all_canonicals = set(ortho_df[ortho_df[\"is_canonical\"]].index.to_list())\n",
    "\n",
    "eprint(\n",
    "    \"We'll work on {} canonicals in {} orthogroups with total {} accessions\".format(\n",
    "        len(all_canonicals), len(orthogroups), len(ortho_df)\n",
    "    )\n",
    ")\n",
    "eprint(\n",
    "    \"\\n*** Part 3 workflow completed {} -- Elapsed: {} --\\n\".format(\n",
    "        time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), elapsed_time(start_secs)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Build alignments and trees, identify clades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True to reprocess those already done\n",
    "FORCE_REPROCESS = False\n",
    "# set to True when debugging for high verbosity of each group\n",
    "VERBOSE_PROCESS = False\n",
    "# set to False to skip checking for all groups to be done\n",
    "CHECK_ALL_PROCESSED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to skip or remove any group (e.g. with issues or bad data)\n",
    "problematic_groups = []\n",
    "for problematic_group in problematic_groups:\n",
    "    if problematic_group in orthogroups:\n",
    "        eprint(\"removed problematic group {}\".format(problematic_group))\n",
    "        orthogroups.remove(problematic_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"skip_reprocessing_orthogroups\"] or len(config[\"groups2run\"]):\n",
    "    # we won't (re)process orthogroups and instead simply re-score, parsing existing n_data files\n",
    "    eprint(\n",
    "        \"\\n*** Part 4 workflow skipped due to skip_reprocessing_orthogroups config option or specified list of groups\\n\"\n",
    "    )\n",
    "else:\n",
    "    start_secs = time.time()\n",
    "    if config[\"threads\"] > 1:\n",
    "        eprint(\n",
    "            \"Working in {} parallel threads; your OS reports {} cpus.\".format(\n",
    "                config[\"threads\"], cpu_count()\n",
    "            )\n",
    "        )\n",
    "        pool = Pool(config[\"threads\"])\n",
    "        iterator = pool.imap(build_tree_for_orthogroup_wrapper, orthogroups)\n",
    "        if config[\"progressbar\"]:\n",
    "            _ = list(tqdm(iterator, total=len(orthogroups)))\n",
    "        else:\n",
    "            iterator\n",
    "        pool.close()  # no more work to submit\n",
    "        pool.join()  # wait workers to finish\n",
    "    else:\n",
    "        iterator = tqdm(orthogroups) if config[\"progressbar\"] else orthogroups\n",
    "        for orthoid in iterator:\n",
    "            build_tree_for_orthogroup_wrapper(orthoid)\n",
    "    eprint(\n",
    "        \"\\n*** Part 4 workflow completed {} -- Elapsed: {}, {} g/s --\\n\".format(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "            *elapsed_time(start_secs, len(orthogroups))\n",
    "        )\n",
    "    )\n",
    "    if CHECK_ALL_PROCESSED:\n",
    "        check_all_files_exist(config[\"semaphores_dir\"], orthogroups, \"done\")\n",
    "\n",
    "\n",
    "if len(config[\"groups2run\"]):\n",
    "    groups_not_found = []\n",
    "    for single_group in config[\"groups2run\"]:\n",
    "        if single_group not in orthogroups:\n",
    "            eprint(\"  ERROR: '{}' not in the df\".format(single_group))\n",
    "            groups_not_found.append(single_group)\n",
    "            continue\n",
    "        if config[\"debug_mode\"]:\n",
    "            eprint(\"  Running group '{}'\".format(single_group))\n",
    "            eprint(\n",
    "                get_orthologs_df_from_pantherid(single_group, ortho_df)\n",
    "                .reset_index()\n",
    "                .sort_values(\n",
    "                    by=[\"org\", \"is_canonical\", \"outlier\", \"entry_type\", \"acc\"],\n",
    "                    ascending=[True, False, False, True, True],\n",
    "                )[\n",
    "                    [\n",
    "                        \"entry_type\",\n",
    "                        \"acc\",\n",
    "                        \"org\",\n",
    "                        \"seqlen\",\n",
    "                        \"groupid\",\n",
    "                        \"is_canonical\",\n",
    "                        \"outlier\",\n",
    "                    ]\n",
    "                ]\n",
    "                .set_index(\"acc\")\n",
    "            )\n",
    "            build_tree_for_orthogroup(\n",
    "                single_group,\n",
    "                ortho_df=ortho_df,\n",
    "                verbose=config[\"debug_mode\"],\n",
    "                debuginfo=config[\"debug_mode\"],\n",
    "                config=config,\n",
    "                get_sequences_fn=get_sequences,\n",
    "            )\n",
    "        else:\n",
    "            build_tree_for_orthogroup(\n",
    "                single_group,\n",
    "                ortho_df=ortho_df,\n",
    "                verbose=config[\"debug_mode\"],\n",
    "                config=config,\n",
    "                get_sequences_fn=get_sequences,\n",
    "            )\n",
    "    config[\"groups2run\"] = [\n",
    "        x for x in config[\"groups2run\"] if x not in groups_not_found\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: parse clades and create output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(config[\"groups2run\"]):\n",
    "    output_all = {key: \"\" for key in config[\"output_keys\"]}\n",
    "    output_data = {}\n",
    "    for key in config[\"output_keys\"]:\n",
    "        output_all[key] = \"\"\n",
    "\n",
    "    for orthoid in config[\"groups2run\"]:\n",
    "        (\n",
    "            output_data[\"gc\"],\n",
    "            output_data[\"changes\"],\n",
    "            output_data[\"confirm\"],\n",
    "            output_data[\"skipped\"],\n",
    "            output_data[\"conflict\"],\n",
    "        ) = scan_ndata_file(\n",
    "            orthoid,\n",
    "            ortho_df=ortho_df,\n",
    "            prevgc_df=prevgc_df,\n",
    "            config=config,\n",
    "            get_sequences_fn=get_sequences,\n",
    "        )\n",
    "        for key in config[\"output_keys\"]:\n",
    "            output_all[key] += output_data[key]\n",
    "\n",
    "    for key in config[\"output_keys\"]:\n",
    "        if output_all[key] != \"\":\n",
    "            print(headers[key], output_all[key], sep=\"\")\n",
    "        else:\n",
    "            eprint(\"NOTICE: no output for {}\".format(key))\n",
    "else:\n",
    "    start_secs = time.time()\n",
    "\n",
    "    if config[\"threads\"] > 1:  # multithread\n",
    "        for key in config[\"output_keys\"]:\n",
    "            # remove leftover tempfiles\n",
    "            clean_up_tempfiles(config[key + \"_outfile\"])\n",
    "        eprint(\n",
    "            \"* Working in {} parallel threads; your OS reports {} CPUs.\".format(\n",
    "                config[\"threads\"], cpu_count()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pool = Pool(config[\"threads\"])\n",
    "        if config[\"progressbar\"]:\n",
    "            _ = list(\n",
    "                tqdm(\n",
    "                    pool.imap(scan_ndata_file_wrapper, orthogroups),\n",
    "                    total=len(orthogroups),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            pool.imap(scan_ndata_file_wrapper, orthogroups)\n",
    "        pool.close()  # no more work to submit\n",
    "        pool.join()  # wait workers to finish\n",
    "\n",
    "        for key in config[\"output_keys\"]:\n",
    "            combine_and_print_output(\n",
    "                config[key + \"_outfile\"],\n",
    "                key,\n",
    "                headers[key],\n",
    "                prevgc_df=prevgc_df,\n",
    "                config=config,\n",
    "            )\n",
    "    else:  # single thread\n",
    "        output_fh = {}  # output filehandles\n",
    "        output_data = {}\n",
    "        for key in config[\"output_keys\"]:\n",
    "            eprint(\"* Creating output file: {}\".format(config[key + \"_outfile\"]))\n",
    "            fh = open(config[key + \"_outfile\"], \"w\")\n",
    "            fh.write(headers[key])\n",
    "            output_fh[key] = fh\n",
    "\n",
    "        if config[\"progressbar\"]:\n",
    "            iterator = tqdm(orthogroups)\n",
    "        else:\n",
    "            iterator = orthogroups\n",
    "\n",
    "        if not prevgc_df.empty:  # prepend the previous suggestions to gc output\n",
    "            dump_prev_changes(output_fh[\"gc\"], prevgc_df, config=config)\n",
    "\n",
    "        for orthoid in iterator:\n",
    "            (\n",
    "                output_data[\"gc\"],\n",
    "                output_data[\"changes\"],\n",
    "                output_data[\"confirm\"],\n",
    "                output_data[\"skipped\"],\n",
    "                output_data[\"conflict\"],\n",
    "            ) = scan_ndata_file(\n",
    "                orthoid,\n",
    "                ortho_df=ortho_df,\n",
    "                prevgc_df=prevgc_df,\n",
    "                config=config,\n",
    "                get_sequences_fn=get_sequences,\n",
    "            )\n",
    "            for key in config[\"output_keys\"]:\n",
    "                if output_data[key] != \"\":\n",
    "                    output_fh[key].write(output_data[key])\n",
    "\n",
    "        # close files\n",
    "        for fh in output_fh.values():\n",
    "            fh.close()\n",
    "\n",
    "    eprint(\n",
    "        \"\\n*** Part 5 workflow completed {} -- Elapsed: {}, {} g/s --\\n\".format(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "            *elapsed_time(start_secs, len(orthogroups))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
